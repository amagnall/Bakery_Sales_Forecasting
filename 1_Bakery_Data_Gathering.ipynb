{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4ba1d1",
   "metadata": {},
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"float:left\"><h1> Forecasting Bakery Sales - Abi Magnall </h1></div>\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:65px\" src =\"https://twomagpiesbakery.co.uk/wp-content/uploads/2020/11/logo-no-site.jpg\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1fcd1",
   "metadata": {},
   "source": [
    "# Notebook 1 : Bakery Data Gathering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878edf8",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explain the problem and purpose of this project, as well as collect the data from the a cloud based system using their API. \n",
    "\n",
    "A total of four bakeries, which will be referred to as `Aldeburgh`, `Southwold`, `Darsham` and `Norwich` throughout this project, will have the transaction data collected spanning from 01-09-2020 to 31-10-2022. This data was provided by the amazing `Two Magpies Bakery` in order to improve their business planning and strategy using the outcomes of this project. \n",
    "\n",
    "**N.B** **This notebook does not need to be run**, it only contains cells to make the API calls to the till system, and therefore will not run without the API tokens. The cells have been commented out incase. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5c0c7",
   "metadata": {},
   "source": [
    "# Contents  \n",
    "\n",
    "**1. [Introduction](#Introduction)**\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "\n",
    "\n",
    "**2. [Data Collection](#Data-Collection)**\n",
    "- [Data and Column Descriptions](#Data-and-Column-Descriptions)\n",
    "\n",
    "**3. [Next Steps](#Next-Steps)**\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c529e",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "## Problem Statement \n",
    "\n",
    "Cash flow is a key metric in determining whether a business is able to grow and survive, particularly during times of economic uncertainty. It is essential for a business to know how much revenue it’s going to generate and, depending on the type of business, when it will receive payments from customers throughout the year so that it can plan any investments accordingly. Without having an accurate data driven forecast, all the planning and strategy would be guesswork. Therefore, it is essential for the success and growth of any business to have a reliable forecast.\n",
    "\n",
    "\n",
    "\n",
    "    \"The two most important things about forecast accuracy are:\n",
    "    1. Surety of cash and so giving confidence to the banks and any potential investors\n",
    "    2. To manage & optimise manufacturing efficiency - ensuring that there is the right labour producing the right product at the right time\" - Co-Owner, Two Magpies Bakery\n",
    "\n",
    "\n",
    "\n",
    "**The Value Add of an Accurate Data-Driven Revenue Forecast:**\n",
    "- Managers can be proactive instead of reactive and can adapt their strategy to achieve the sales needed\n",
    "- Better investments, staffing and hiring decisions based on peaks and troughs throughout the year (achieved through a weekly and monthly forecast) \n",
    "- A monthly forecast is required to show to potential investors to raise funding as it shows that the business if not only target but going to grow\n",
    "\n",
    "All of these are required to support the growth of the business. \n",
    "\n",
    "**The Aim of this Project:**\n",
    "\n",
    "The aim of this project is to develop accurate revenue forecasts for a leading Artisan Bakery in East Anglia, called the [Two Magpies Bakery](https://twomagpiesbakery.co.uk/). The bakery requires: \n",
    "1. A daily forecast up to 7 days ahead, *with a target accuracy of 95%*\n",
    "2. A weekly forecast up to 6 weeks ahead, *with a target accuracy of 95%*\n",
    "3. A monthly forecast up to 6 months ahead, *with a target accuracy of 92-95%* \n",
    "\n",
    "\n",
    "**How to Achieve This Project:**\n",
    "\n",
    "Through thorough research, the modelling plan for this project includes: \n",
    "- Moving Average Model, which will be the baseline model as it is the simplest to implement and most understandable \n",
    "- Linear Regression Model, another simple and easy to implement and understand model, that can perform successfully depending on the scenario \n",
    "- SARIMAX, which is known in industry as one of the leading timeseries models\n",
    "- Facebook Prophet, also known in industry as one of the leading timeseries algorithms and is particularly good at dealing with special dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcd85e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b0d26",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "The data was collected calling an API to the EposNow till system used by the bakery. The API tokens are saved in external text files which are called in, therefore the below code blocks performing the API call will not run without the tokens. \n",
    "\n",
    "The API gets the data between two given dates, which gets appended to a dataframe. The newly formed dataframe has rows of data, where each row of data is a new transaction. \n",
    "\n",
    "The full raw dataset runs from 01/09/2020-31/10/2022. These dates were selected as it the longest amount of time that was available post the main lockdowns due to the Corona virus pandemic. This size of data is the minimum required for timeseries analysis, and potentially may prove problematic due to the length. However, speaking with the bakery owner, the sales and spending habits of customers have completely changed since the pandemic. Therefore, it was thought that collecting data pre-pandemic would have different revenue trends that would be identified and forecasted by the models, which are not longer accurate for the true patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4844c8e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6954cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb69327",
   "metadata": {},
   "source": [
    "## To Get Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98287f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "working_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d471f",
   "metadata": {},
   "source": [
    "## Importing Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a90edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import BakeryFunctions as bakery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b7037",
   "metadata": {},
   "source": [
    "## Importing the API Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62278ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ald_token = open('aldeburgh_token.txt', 'r')\n",
    "sw_token = open('southwold_token.txt', 'r')\n",
    "dar_token = open('darsham_token.txt', 'r')\n",
    "nor_token = open('norwich_token.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c18700",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea85755",
   "metadata": {},
   "source": [
    "# Aldeburgh Data Download \n",
    "*The inputted data is for the final download of data which was for the test set spanning 01-10-2022 - 01-11-2022, not the full data set from 01-09-2020.*\n",
    "\n",
    "The raw data is returned as a list of dictionaries, where each dictionary contains the transaction information. This is identified and retrieved from the full list and gets appended to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797ffcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Aldeburgh Download\n",
    "\n",
    "# # An empty dataframe is created to be filled with transaction data \n",
    "# aldeburgh_df = pd.DataFrame()\n",
    "\n",
    "# # To loop through the page numbers \n",
    "# for i in range(1, 65): \n",
    "#     # Link to call the API which gets the transaction rows between two dates \n",
    "#     api_url = 'https://api.eposnowhq.com/api/v4/Transaction/GetByDate/2022-10-01/2022-11-01/?page='+str(i)\n",
    "#     # To call the API token \n",
    "#     token = ald_token.read()\n",
    "#     headers = {'Authorization': \"Basic {} \".format(token)}\n",
    "#     auth_response = requests.get(api_url, headers=headers)\n",
    "#     print(f'Processing page {i}: {len(auth_response.json())} transactions found.')\n",
    "#     for j in range(200):\n",
    "#         try:\n",
    "#             # To append the data in the correct format, with the correct date for each transaction \n",
    "#             for transaction_item in auth_response.json()[j]['TransactionItems']:\n",
    "#                 tmp = pd.DataFrame.from_dict(transaction_item, orient = 'index').T\n",
    "#                 tmp['Date'] = auth_response.json()[j]['DateTime']\n",
    "#                 #print(tmp)\n",
    "#                 try:\n",
    "#                     aldeburgh_df = pd.concat([aldeburgh_df, tmp])\n",
    "#                 except:\n",
    "#                     # To print if there are any issues processing that transaction row \n",
    "#                     print(f'Issue processing transaction: page {i}, transaction {j}')\n",
    "#         except:\n",
    "#             print(f'Issue processing transaction: page {i}, transaction {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bf7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # To validate it downloaded successfully \n",
    "# aldeburgh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To export the the data to a new csv file \n",
    "# aldeburgh_df.to_csv(working_directory+'/1_raw_data/ald_oct22.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a68701",
   "metadata": {},
   "source": [
    "## Observations \n",
    "Whilst the data was being called from the API, some transactions had issues processing them. This will therefore be explored in [Bakery Data Preprocessing](./4_Bakery_Data_Preprocessing.ipynb) to ensure that all transaction rows are accounted for but comapring the calculated revenue to the true revenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2c3ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378f0e2",
   "metadata": {},
   "source": [
    "# Southwold Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58324b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Southwold Download\n",
    "\n",
    "# # A new empty dataframe is created \n",
    "# southwold_df = pd.DataFrame()\n",
    "# # To loop through the page numbers \n",
    "# for i in range(1, 65): \n",
    "#     # To call the API \n",
    "#     api_url = 'https://api.eposnowhq.com/api/v4/Transaction/GetByDate/2022-10-01/2022-11-01/?page='+str(i)\n",
    "#     token = sw_token.read()\n",
    "#     headers = {'Authorization': \"Basic {} \".format(token)}\n",
    "#     auth_response = requests.get(api_url, headers=headers)\n",
    "#     print(f'Processing page {i}: {len(auth_response.json())} transactions found.')\n",
    "#     for j in range(200):\n",
    "#         try:\n",
    "#             for transaction_item in auth_response.json()[j]['TransactionItems']:\n",
    "#                 tmp = pd.DataFrame.from_dict(transaction_item, orient = 'index').T\n",
    "#                 tmp['Date'] = auth_response.json()[j]['DateTime']\n",
    "#                 try:\n",
    "#                     southwold_df = pd.concat([southwold_df, tmp])\n",
    "#                 except:\n",
    "#                     print(f'Issue processing transaction: page {i}, transaction {j}')\n",
    "#         except:\n",
    "#             print(f'Issue processing transaction: page {i}, transaction {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To validate it worked \n",
    "# southwold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To save the raw data to a csv file \n",
    "# southwold_df.to_csv(working_directory+'/1_raw_data/sw_oct22.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838d061",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56397d9",
   "metadata": {},
   "source": [
    "# Darsham Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Darsham Download\n",
    "\n",
    "# # Empty dataframe is created\n",
    "# darsham_df = pd.DataFrame()\n",
    "# # To loop through the page numbers \n",
    "# for i in range(1, 65): \n",
    "#     api_url = 'https://api.eposnowhq.com/api/v4/Transaction/GetByDate/2022-10-01/2022-11-01/?page='+str(i)\n",
    "#     token = dar_token.read()\n",
    "#     headers = {'Authorization': \"Basic {} \".format(token)}\n",
    "#     auth_response = requests.get(api_url, headers=headers)\n",
    "#     print(f'Processing page {i}: {len(auth_response.json())} transactions found.')\n",
    "#     for j in range(200):\n",
    "#         try:\n",
    "#             for transaction_item in auth_response.json()[j]['TransactionItems']:\n",
    "#                 tmp = pd.DataFrame.from_dict(transaction_item, orient = 'index').T\n",
    "#                 tmp['Date'] = auth_response.json()[j]['DateTime']\n",
    "#                 #print(tmp)\n",
    "#                 try:\n",
    "#                     darsham_df = pd.concat([darsham_df, tmp])\n",
    "#                 except:\n",
    "#                     print(f'Issue processing transaction: page {i}, transaction {j}')\n",
    "#         except:\n",
    "#             print(f'Issue processing transaction: page {i}, transaction {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To validate it worked \n",
    "# darsham_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the raw data to csv \n",
    "darsham_df.to_csv(working_directory+'/1_raw_data/dars_oct22.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40619e7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2f326",
   "metadata": {},
   "source": [
    "# Norwich Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Norwich Download\n",
    "\n",
    "# # New empty dataframe is created\n",
    "# norwich_df = pd.DataFrame()\n",
    "# # To loop through the page numbers \n",
    "# for i in range(1, 65): \n",
    "#     api_url = 'https://api.eposnowhq.com/api/v4/Transaction/GetByDate/2022-10-01/2022-11-01/?page='+str(i)\n",
    "#     token = nor_token\n",
    "#     headers = {'Authorization': \"Basic {} \".format(token)}\n",
    "#     auth_response = requests.get(api_url, headers=headers)\n",
    "#     print(f'Processing page {i}: {len(auth_response.json())} transactions found.')\n",
    "#     for j in range(200):\n",
    "#         try:\n",
    "#             for transaction_item in auth_response.json()[j]['TransactionItems']:\n",
    "#                 tmp = pd.DataFrame.from_dict(transaction_item, orient = 'index').T\n",
    "#                 tmp['Date'] = auth_response.json()[j]['DateTime']\n",
    "#                 #print(tmp)\n",
    "#                 try:\n",
    "#                     norwich_df = pd.concat([norwich_df, tmp])\n",
    "#                 except:\n",
    "#                     print(f'Issue processing transaction: page {i}, transaction {j}')\n",
    "#         except:\n",
    "#             print(f'Issue processing transaction: page {i}, transaction {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c546d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # To validate it worked\n",
    "# norwich_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To save the raw data to csv\n",
    "# norwich_df.to_csv(working_directory+'/1_raw_data/nor_oct22.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d44a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8777c7",
   "metadata": {},
   "source": [
    "# Data and Column Descriptions\n",
    "Basic EDA will be carried out on one of the dataframes to identify what format the raw data is collected in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb98e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop the two columns that contains lists for each row\n",
    "# darsham_df_eda = darsham_df.drop(columns=['Taxes', 'MultipleChoiceItems']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08318b4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bakery.further_eda(darsham_df_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b6122",
   "metadata": {},
   "source": [
    "## Observations \n",
    "From the above intital EDA, it can be seen that:\n",
    "- The raw data consists of multiple rows of data (c.20,000 for this API call but over 500,000 for the full raw data) and 23 columns. \n",
    "- There are 4 columns that are completely empty, which are likely to be redundant and be removed in the cleaning process. \n",
    "- There are a lot of missing values that need to be dealt with appropriately. \n",
    "- There appears to be no dupliacted rows of data, but this is on a small sample of the true full dataset so this will be re-assessed in the cleaning process.\n",
    "- All the datatypes are object, which will have to be amended to the approprate datatype in the cleaning process.\n",
    "\n",
    "A data dictionary for the raw data can be found below: \n",
    "\n",
    "|Column| Description | \n",
    "|:--| :- | \n",
    "|Id    |   The ID of the row of data from the till system  |                \n",
    "|TransactionId| The unique transaction ID | \n",
    "|ProductId     |  The ID of of the product(s) bought |                \n",
    "|UnitPrice      |       The unit price of the product(s) bought |         \n",
    "|UnitPriceExcTax |     The unit price of the product(s) bought excluding tax |           \n",
    "|CostPrice        |    The cost price of the product(s) that is stored in the system |           \n",
    "|TaxGroupId        |  The tax group ID of the product(s) |\n",
    "|Quantity         |    The quantity of the product bought in that transaction |         \n",
    "|DiscountAmount  |    The discount amount on the transaction |        \n",
    "|DiscountReasonId |     The ID of the discount reason on the transaction | \n",
    "|DiscountAmountExcTax|  The discount amount excluding tax |      \n",
    "|RefundReasonId     |    The ID of the refund reason |     \n",
    "|Notes               |   Additional notes added to the order |     \n",
    "|PrintOnOrder         |    Boolean if the order was printed |       \n",
    "|MultipleChoiceProductId | Feature of the till system not utilised by the bakery |   \n",
    "|ParentId       |   Feature of the till system not utilised by the bakery |          \n",
    "|IsTaxExempt     | Boolean if the order is tax exempt or not |               \n",
    "|MeasurementDetails |  Feature of the till system not utilised by the bakery |\n",
    "|Taxes| List of dictionaries containing Tax information | \n",
    "|MultipleChoiceItems| List of dictionaries containing the selection of different flavours available of that product | \n",
    "|CourseFired         |   Feature of the till system not utilised by the bakery |        \n",
    "|Date                 | The date of the transaction |           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6e2fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccad568",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "- The weather data will also be cleaned and explored to ensure it is clean, in the correct and required format for the modelling phase, this can be found here [Weather Cleaning and EDA](./2_Weather_Cleaning_EDA.ipynb). \n",
    "\n",
    "- The raw bakery transaction data needs to be explored for missing, duplicated or erronous data and cleaned to remove redundant columns. This is carried out in [Bakery Data Cleaning Notebook](./3_Bakery_Data_Cleaning.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56d55d",
   "metadata": {},
   "source": [
    ">[Return to Contents](#Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bakery",
   "language": "python",
   "name": "bakery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
